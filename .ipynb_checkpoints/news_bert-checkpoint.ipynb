{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'finBERT'...\n",
      "remote: Enumerating objects: 73, done.\u001b[K\n",
      "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
      "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
      "remote: Total 73 (delta 26), reused 55 (delta 17), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (73/73), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ProsusAI/finBERT.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://newsapi-dataset/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://luckykcw/cleaned_all.tsv\n",
      "gs://luckykcw/cleaned_unlabel.csv\n",
      "gs://luckykcw/cleaned_w_embed.tsv\n",
      "gs://luckykcw/intent.csv\n",
      "gs://luckykcw/vocab.txt\n",
      "gs://luckykcw/fake-news-pair-classification-challenge/\n",
      "gs://luckykcw/model_berttokenizer/\n",
      "gs://luckykcw/news/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://luckykcw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://clf_model/first64tokenep20/vocab.txt [Content-Type=text/plain]...\n",
      "Copying file://clf_model/first64tokenep20/special_tokens_map.json [Content-Type=application/json]...\n",
      "Copying file://clf_model/first64tokenep20/tokenizer_config.json [Content-Type=application/json]...\n",
      "Copying file://clf_model/first64tokenep20/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "- [4 files][417.9 MiB/417.9 MiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file://clf_model/first64tokenep20/config.json [Content-Type=application/json]...\n",
      "Copying file://clf_model/first32tokenep20newsplit/vocab.txt [Content-Type=text/plain]...\n",
      "Copying file://clf_model/first32tokenep20newsplit/special_tokens_map.json [Content-Type=application/json]...\n",
      "Copying file://clf_model/first32tokenep20newsplit/tokenizer_config.json [Content-Type=application/json]...\n",
      "Copying file://clf_model/first32tokenep20newsplit/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file://clf_model/first32tokenep20newsplit/config.json [Content-Type=application/json]...\n",
      "Copying file://clf_model/first32tokenep20newsplit/.ipynb_checkpoints/config-checkpoint.json [Content-Type=application/json]...\n",
      "/ [11 files][835.8 MiB/835.8 MiB]   77.1 MiB/s                                  \n",
      "Operation completed over 11 objects/835.8 MiB.                                   \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r clf_model gs://luckykcw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"newsapi-dataset/newsapi_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>uid</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>description</th>\n",
       "      <th>publishedat</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>urltoimage</th>\n",
       "      <th>storyid</th>\n",
       "      <th>insert_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>761317</td>\n",
       "      <td>Zameena Mejia</td>\n",
       "      <td>\"Anything that acts as a catalyst for self-ref...</td>\n",
       "      <td>According to bestselling author and happiness ...</td>\n",
       "      <td>2017-12-31 01:00:00</td>\n",
       "      <td>cnbc</td>\n",
       "      <td>Here's how to tell if you'll succeed at your N...</td>\n",
       "      <td>https://www.cnbc.com/2017/12/29/heres-how-to-t...</td>\n",
       "      <td>https://fm.cnbc.com/applications/cnbc.com/reso...</td>\n",
       "      <td>https://www.cnbc.com/2017/12/29/heres-how-to-t...</td>\n",
       "      <td>2019-09-11 22:52:28.201172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>690810</td>\n",
       "      <td>Sam Meredith</td>\n",
       "      <td>Members of the United Nations Security Council...</td>\n",
       "      <td>President Donald Trump warned Russia of immine...</td>\n",
       "      <td>2018-04-11 21:36:00</td>\n",
       "      <td>cnbc</td>\n",
       "      <td>Syrian war explained: Here's everything you ne...</td>\n",
       "      <td>https://www.cnbc.com/2018/04/11/syrian-war-exp...</td>\n",
       "      <td>https://fm.cnbc.com/applications/cnbc.com/reso...</td>\n",
       "      <td>https://www.cnbc.com/2018/04/11/syrian-war-exp...</td>\n",
       "      <td>2019-09-11 21:56:50.776935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>690815</td>\n",
       "      <td>Sara Salinas</td>\n",
       "      <td>Mark Zuckerberg is testifying before the House...</td>\n",
       "      <td>Zuckerberg is testifying before the House Ener...</td>\n",
       "      <td>2018-04-11 21:29:00</td>\n",
       "      <td>cnbc</td>\n",
       "      <td>Watch Mark Zuckerberg's second day of testimon...</td>\n",
       "      <td>https://www.cnbc.com/2018/04/11/watch-mark-zuc...</td>\n",
       "      <td>https://fm.cnbc.com/applications/cnbc.com/reso...</td>\n",
       "      <td>https://www.cnbc.com/2018/04/11/watch-mark-zuc...</td>\n",
       "      <td>2019-09-11 21:56:50.776935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     uid         author  \\\n",
       "0           0  761317  Zameena Mejia   \n",
       "1           1  690810   Sam Meredith   \n",
       "2           2  690815   Sara Salinas   \n",
       "\n",
       "                                             content  \\\n",
       "0  \"Anything that acts as a catalyst for self-ref...   \n",
       "1  Members of the United Nations Security Council...   \n",
       "2  Mark Zuckerberg is testifying before the House...   \n",
       "\n",
       "                                         description          publishedat  \\\n",
       "0  According to bestselling author and happiness ...  2017-12-31 01:00:00   \n",
       "1  President Donald Trump warned Russia of immine...  2018-04-11 21:36:00   \n",
       "2  Zuckerberg is testifying before the House Ener...  2018-04-11 21:29:00   \n",
       "\n",
       "  source                                              title  \\\n",
       "0   cnbc  Here's how to tell if you'll succeed at your N...   \n",
       "1   cnbc  Syrian war explained: Here's everything you ne...   \n",
       "2   cnbc  Watch Mark Zuckerberg's second day of testimon...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.cnbc.com/2017/12/29/heres-how-to-t...   \n",
       "1  https://www.cnbc.com/2018/04/11/syrian-war-exp...   \n",
       "2  https://www.cnbc.com/2018/04/11/watch-mark-zuc...   \n",
       "\n",
       "                                          urltoimage  \\\n",
       "0  https://fm.cnbc.com/applications/cnbc.com/reso...   \n",
       "1  https://fm.cnbc.com/applications/cnbc.com/reso...   \n",
       "2  https://fm.cnbc.com/applications/cnbc.com/reso...   \n",
       "\n",
       "                                             storyid  \\\n",
       "0  https://www.cnbc.com/2017/12/29/heres-how-to-t...   \n",
       "1  https://www.cnbc.com/2018/04/11/syrian-war-exp...   \n",
       "2  https://www.cnbc.com/2018/04/11/watch-mark-zuc...   \n",
       "\n",
       "                  insert_time  \n",
       "0  2019-09-11 22:52:28.201172  \n",
       "1  2019-09-11 21:56:50.776935  \n",
       "2  2019-09-11 21:56:50.776935  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['content'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=373, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing, original_str, normalized_str])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(df['content'].iloc[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536366"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把重複的刪掉 (bot-related)少了20多萬筆\n",
    "# 例如：有很多內文其實是防止爬蟲的回應 ex: \"To continue, please click the box below...\"\n",
    "text_dropdup = list(set(text))\n",
    "len(text_dropdup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = [len(t.split()) for t in text_dropdup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428.24443"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(order[300000:400000])/len(order[300000:400000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12313"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1080"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(text_dropdup[argorder[500000]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "argorder = np.argsort(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307519"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argorder[300000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.sort(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_prep = [\" \".join(nlp_preprocessing_tools.preprocess(x)) for x in text_dropdup]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use nlp preprocessing tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://kcwanglucky:[password]@github.com/peihsuan/proj_ai_report.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r proj_ai_report/model/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('proj_ai_report/model/')\n",
    "import nlp_preprocessing_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = [\" \".join(nlp_preprocessing_tools.preprocess(str(t))) for t in text_dropdup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sample(text_dropdup, 10000)\n",
    "test = list(set(train) - set(sample(train, 9000)))\n",
    "train = list(set(train) - set(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trainfile\", \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"evalfile\", \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Use pretrained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a english tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dirname = \"bert_uncased_tokenizer\"\n",
    "if not os.path.isdir(dirname):\n",
    "    os.mkdir(dirname)\n",
    "tokenizer.save_pretrained(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newstokenizer_dirname = \"newstokenizer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Train a tokenizer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要特別注意的是，自行train出來的BertWordPieceTokenizer沒有包含config.jason。此檔註明do_lower_case:true\n",
    "max_len:512。若沒有設maxlen=512，run_language_model無法執行，因為tokenizer的max_len被設成10000000所以每個blocksizek為負"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "tokenizer.train(\n",
    "    \"trainfile\",\n",
    "    vocab_size=50000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer/newstokenizer0/vocab.txt']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=\"trainfile\", min_frequency=2)\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "# Save files to disk\n",
    "dirname = os.path.join(\"tokenizer\", \"newstokenizer0\")\n",
    "if not os.path.isdir(dirname):\n",
    "    os.mkdir(dirname)\n",
    "tokenizer.save(dirname)\n",
    "\n",
    "# # And then encode:\n",
    "# encoded = tokenizer.encode(\"Maduro’s socialist administration said it had closed the country’s maritime border with the Dutch Caribbean islands of Aruba, Curacao and Bonaire, after Curacao’s government said it would help store aid destined for Venezuela.  Reporting by Angus Berwick; Editing by Bernadette Baum\")\n",
    "# print(encoded.ids)\n",
    "# print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(dirname)\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)#.max_len_single_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Train a bert model from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-11 09:33:19.887345: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\n",
      "2020-03-11 09:33:19.889274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\n",
      "03/11/2020 09:33:24 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "03/11/2020 09:33:24 - INFO - transformers.tokenization_utils -   Model name 'tokenizer/newstokenizer1_vsize50000/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'tokenizer/newstokenizer1_vsize50000/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "03/11/2020 09:33:24 - INFO - transformers.tokenization_utils -   Didn't find file tokenizer/newstokenizer1_vsize50000/added_tokens.json. We won't load it.\n",
      "03/11/2020 09:33:24 - INFO - transformers.tokenization_utils -   Didn't find file tokenizer/newstokenizer1_vsize50000/special_tokens_map.json. We won't load it.\n",
      "03/11/2020 09:33:24 - INFO - transformers.tokenization_utils -   loading file tokenizer/newstokenizer1_vsize50000/vocab.txt\n",
      "03/11/2020 09:33:24 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/11/2020 09:33:24 - INFO - transformers.tokenization_utils -   loading file None\n",
      "03/11/2020 09:33:24 - INFO - transformers.tokenization_utils -   loading file tokenizer/newstokenizer1_vsize50000/tokenizer_config.json\n",
      "03/11/2020 09:33:24 - INFO - __main__ -   Training new model from scratch\n",
      "03/11/2020 09:33:29 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='evalfile', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path=None, model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='./bert_model/newsbert_owntokenizer_vsize5w_woblocksize', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name='tokenizer/newstokenizer1_vsize50000/', train_data_file='trainfile', warmup_steps=0, weight_decay=0.0)\n",
      "03/11/2020 09:33:29 - INFO - __main__ -   Loading features from cached file bert_cached_lm_510_trainfile\n",
      "03/11/2020 09:33:29 - INFO - __main__ -   ***** Running training *****\n",
      "03/11/2020 09:33:29 - INFO - __main__ -     Num examples = 8796\n",
      "03/11/2020 09:33:29 - INFO - __main__ -     Num Epochs = 1\n",
      "03/11/2020 09:33:29 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "03/11/2020 09:33:29 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "03/11/2020 09:33:29 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "03/11/2020 09:33:29 - INFO - __main__ -     Total optimization steps = 2199\n",
      "Epoch:   0%|                                              | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                       | 0/2199 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   0%|                               | 1/2199 [00:01<49:37,  1.35s/it]\u001b[A\n",
      "Iteration:   0%|                               | 2/2199 [00:02<46:46,  1.28s/it]\u001b[A\n",
      "Iteration:   0%|                               | 3/2199 [00:03<44:34,  1.22s/it]\u001b[A\n",
      "Iteration:   0%|                               | 4/2199 [00:04<42:46,  1.17s/it]\u001b[A\n",
      "Iteration:   0%|                               | 5/2199 [00:05<41:39,  1.14s/it]\u001b[A\n",
      "Iteration:   0%|                               | 6/2199 [00:06<40:52,  1.12s/it]\u001b[A\n",
      "Iteration:   0%|                               | 7/2199 [00:07<40:25,  1.11s/it]\u001b[A\n",
      "Iteration:   0%|                               | 8/2199 [00:08<40:01,  1.10s/it]\u001b[A\n",
      "Iteration:   0%|▏                              | 9/2199 [00:09<39:32,  1.08s/it]\u001b[A\n",
      "Iteration:   0%|▏                             | 10/2199 [00:11<39:23,  1.08s/it]\u001b[A\n",
      "Iteration:   1%|▏                             | 11/2199 [00:12<39:12,  1.08s/it]\u001b[A\n",
      "Iteration:   1%|▏                             | 12/2199 [00:13<39:11,  1.08s/it]\u001b[A\n",
      "Iteration:   1%|▏                             | 13/2199 [00:14<39:09,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|▏                             | 14/2199 [00:15<39:03,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|▏                             | 15/2199 [00:16<38:55,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|▏                             | 16/2199 [00:17<38:46,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|▏                             | 17/2199 [00:18<38:43,  1.06s/it]\u001b[A\n",
      "Iteration:   1%|▏                             | 18/2199 [00:19<38:53,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|▎                             | 19/2199 [00:20<38:47,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|▎                             | 20/2199 [00:21<38:51,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|▎                             | 21/2199 [00:22<38:56,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|▎                             | 22/2199 [00:23<38:53,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|▎                             | 23/2199 [00:24<38:54,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|▎                             | 24/2199 [00:25<38:49,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|▎                             | 25/2199 [00:27<38:58,  1.08s/it]\u001b[A\n",
      "Iteration:   1%|▎                             | 26/2199 [00:28<38:55,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|▎                             | 27/2199 [00:29<38:56,  1.08s/it]\u001b[A\n",
      "Iteration:   1%|▍                             | 28/2199 [00:30<38:51,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|▍                             | 29/2199 [00:31<38:53,  1.08s/it]\u001b[A\n",
      "Iteration:   1%|▍                             | 30/2199 [00:32<38:49,  1.07s/it]\u001b[A\n",
      "Iteration:   1%|▍                             | 31/2199 [00:33<38:52,  1.08s/it]\u001b[A\n",
      "Iteration:   1%|▍                             | 32/2199 [00:34<38:45,  1.07s/it]\u001b[A\n",
      "Iteration:   2%|▍                             | 33/2199 [00:35<38:42,  1.07s/it]\u001b[A\n",
      "Iteration:   2%|▍                             | 34/2199 [00:36<38:44,  1.07s/it]\u001b[A\n",
      "Iteration:   2%|▍                             | 35/2199 [00:37<38:42,  1.07s/it]\u001b[A\n",
      "Iteration:   2%|▍                             | 36/2199 [00:38<38:48,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▌                             | 37/2199 [00:39<38:49,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▌                             | 38/2199 [00:41<38:51,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▌                             | 39/2199 [00:42<38:41,  1.07s/it]\u001b[A\n",
      "Iteration:   2%|▌                             | 40/2199 [00:43<38:46,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▌                             | 41/2199 [00:44<38:48,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▌                             | 42/2199 [00:45<38:45,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▌                             | 43/2199 [00:46<38:38,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▌                             | 44/2199 [00:47<38:37,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▌                             | 45/2199 [00:48<38:42,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▋                             | 46/2199 [00:49<38:43,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▋                             | 47/2199 [00:50<38:45,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▋                             | 48/2199 [00:51<38:45,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▋                             | 49/2199 [00:52<38:43,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▋                             | 50/2199 [00:53<38:42,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▋                             | 51/2199 [00:55<38:41,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▋                             | 52/2199 [00:56<38:37,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▋                             | 53/2199 [00:57<38:37,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▋                             | 54/2199 [00:58<38:42,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▊                             | 55/2199 [00:59<38:41,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▊                             | 56/2199 [01:00<38:41,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▊                             | 57/2199 [01:01<38:42,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▊                             | 58/2199 [01:02<38:39,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▊                             | 59/2199 [01:03<38:38,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▊                             | 60/2199 [01:04<38:33,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▊                             | 61/2199 [01:05<38:36,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▊                             | 62/2199 [01:06<38:33,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▊                             | 63/2199 [01:08<38:30,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▊                             | 64/2199 [01:09<38:24,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▉                             | 65/2199 [01:10<38:25,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▉                             | 66/2199 [01:11<38:28,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▉                             | 67/2199 [01:12<38:32,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▉                             | 68/2199 [01:13<38:34,  1.09s/it]\u001b[A\n",
      "Iteration:   3%|▉                             | 69/2199 [01:14<38:35,  1.09s/it]\u001b[A\n",
      "Iteration:   3%|▉                             | 70/2199 [01:15<38:28,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▉                             | 71/2199 [01:16<38:27,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▉                             | 72/2199 [01:17<38:23,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▉                             | 73/2199 [01:18<38:27,  1.09s/it]\u001b[A\n",
      "Iteration:   3%|█                             | 74/2199 [01:19<38:20,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|█                             | 75/2199 [01:21<38:22,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|█                             | 76/2199 [01:22<38:28,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|█                             | 77/2199 [01:23<38:25,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|█                             | 78/2199 [01:24<38:19,  1.08s/it]\u001b[A\n",
      "Iteration:   4%|█                             | 79/2199 [01:25<38:17,  1.08s/it]\u001b[A\n",
      "Iteration:   4%|█                             | 80/2199 [01:26<38:09,  1.08s/it]\u001b[A\n",
      "Iteration:   4%|█                             | 81/2199 [01:27<38:12,  1.08s/it]\u001b[A\n",
      "Iteration:   4%|█                             | 82/2199 [01:28<38:16,  1.08s/it]\u001b[A\n",
      "Iteration:   4%|█▏                            | 83/2199 [01:29<38:16,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|█▏                            | 84/2199 [01:30<38:09,  1.08s/it]\u001b[A\n",
      "Iteration:   4%|█▏                            | 85/2199 [01:31<38:12,  1.08s/it]\u001b[A\n",
      "Iteration:   4%|█▏                            | 86/2199 [01:33<38:15,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|█▏                            | 87/2199 [01:34<38:10,  1.08s/it]\u001b[A\n",
      "Iteration:   4%|█▏                            | 88/2199 [01:35<38:17,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|█▏                            | 89/2199 [01:36<38:18,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|█▏                            | 90/2199 [01:37<38:15,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|█▏                            | 91/2199 [01:38<38:12,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|█▎                            | 92/2199 [01:39<38:14,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|█▎                            | 93/2199 [01:40<38:13,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|█▎                            | 94/2199 [01:41<38:12,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|█▎                            | 95/2199 [01:42<38:05,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|█▎                            | 96/2199 [01:43<38:02,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|█▎                            | 97/2199 [01:44<38:04,  1.09s/it]\u001b[A\n",
      "Iteration:   4%|█▎                            | 98/2199 [01:46<38:06,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▎                            | 99/2199 [01:47<38:08,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▎                           | 100/2199 [01:48<38:05,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▎                           | 101/2199 [01:49<38:01,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▎                           | 102/2199 [01:50<38:04,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▎                           | 103/2199 [01:51<37:59,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▎                           | 104/2199 [01:52<37:59,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▍                           | 105/2199 [01:53<37:59,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▍                           | 106/2199 [01:54<37:56,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▍                           | 107/2199 [01:55<37:56,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▍                           | 108/2199 [01:56<38:01,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▍                           | 109/2199 [01:58<38:03,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▍                           | 110/2199 [01:59<37:59,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▍                           | 111/2199 [02:00<38:02,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▍                           | 112/2199 [02:01<38:01,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▍                           | 113/2199 [02:02<37:58,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▌                           | 114/2199 [02:03<37:57,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▌                           | 115/2199 [02:04<37:48,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▌                           | 116/2199 [02:05<37:51,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▌                           | 117/2199 [02:06<37:50,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▌                           | 118/2199 [02:07<37:43,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▌                           | 119/2199 [02:08<37:39,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|█▌                           | 120/2199 [02:10<37:39,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▌                           | 121/2199 [02:11<37:39,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▌                           | 122/2199 [02:12<37:42,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▌                           | 123/2199 [02:13<37:39,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 124/2199 [02:14<37:40,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 125/2199 [02:15<37:37,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 126/2199 [02:16<37:40,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 127/2199 [02:17<37:37,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 128/2199 [02:18<37:36,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 129/2199 [02:19<37:32,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 130/2199 [02:20<37:35,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 131/2199 [02:22<37:37,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▋                           | 132/2199 [02:23<37:32,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 133/2199 [02:24<37:33,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 134/2199 [02:25<37:30,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 135/2199 [02:26<37:25,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 136/2199 [02:27<37:26,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 137/2199 [02:28<37:29,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 138/2199 [02:29<37:22,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 139/2199 [02:30<37:21,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 140/2199 [02:31<37:18,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 141/2199 [02:32<37:18,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|█▊                           | 142/2199 [02:33<37:19,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|█▉                           | 143/2199 [02:35<37:10,  1.08s/it]\u001b[A\n",
      "Iteration:   7%|█▉                           | 144/2199 [02:36<37:07,  1.08s/it]\u001b[A\n",
      "Iteration:   7%|█▉                           | 145/2199 [02:37<37:09,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|█▉                           | 146/2199 [02:38<37:06,  1.08s/it]\u001b[A\n",
      "Iteration:   7%|█▉                           | 147/2199 [02:39<37:11,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|█▉                           | 148/2199 [02:40<37:08,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|█▉                           | 149/2199 [02:41<37:03,  1.08s/it]\u001b[A\n",
      "Iteration:   7%|█▉                           | 150/2199 [02:42<36:59,  1.08s/it]\u001b[A\n",
      "Iteration:   7%|█▉                           | 151/2199 [02:43<36:59,  1.08s/it]\u001b[A\n",
      "Iteration:   7%|██                           | 152/2199 [02:44<37:01,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|██                           | 153/2199 [02:45<36:58,  1.08s/it]\u001b[A\n",
      "Iteration:   7%|██                           | 154/2199 [02:47<37:04,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|██                           | 155/2199 [02:48<37:06,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|██                           | 156/2199 [02:49<37:05,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|██                           | 157/2199 [02:50<37:01,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|██                           | 158/2199 [02:51<37:03,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|██                           | 159/2199 [02:52<37:04,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|██                           | 160/2199 [02:53<37:03,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|██                           | 161/2199 [02:54<36:59,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|██▏                          | 162/2199 [02:55<37:02,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|██▏                          | 163/2199 [02:56<37:01,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|██▏                          | 164/2199 [02:57<36:50,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▏                          | 165/2199 [02:58<36:50,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▏                          | 166/2199 [03:00<36:49,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▏                          | 167/2199 [03:01<36:44,  1.08s/it]\u001b[A\n",
      "Iteration:   8%|██▏                          | 168/2199 [03:02<36:48,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▏                          | 169/2199 [03:03<36:48,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▏                          | 170/2199 [03:04<36:50,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 171/2199 [03:05<36:43,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 172/2199 [03:06<36:42,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 173/2199 [03:07<36:40,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 174/2199 [03:08<36:37,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 175/2199 [03:09<36:42,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 176/2199 [03:10<36:39,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 177/2199 [03:12<36:38,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 178/2199 [03:13<36:34,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 179/2199 [03:14<36:31,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▎                          | 180/2199 [03:15<36:34,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▍                          | 181/2199 [03:16<36:36,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▍                          | 182/2199 [03:17<36:35,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▍                          | 183/2199 [03:18<36:35,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▍                          | 184/2199 [03:19<36:36,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▍                          | 185/2199 [03:20<36:36,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|██▍                          | 186/2199 [03:21<36:38,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▍                          | 187/2199 [03:22<36:30,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▍                          | 188/2199 [03:24<36:30,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▍                          | 189/2199 [03:25<36:33,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 190/2199 [03:26<36:31,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 191/2199 [03:27<36:27,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 192/2199 [03:28<36:25,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 193/2199 [03:29<36:23,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 194/2199 [03:30<36:19,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 195/2199 [03:31<36:20,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 196/2199 [03:32<36:20,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 197/2199 [03:33<36:21,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 198/2199 [03:34<36:15,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▌                          | 199/2199 [03:35<36:13,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 200/2199 [03:37<36:15,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 201/2199 [03:38<36:14,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 202/2199 [03:39<36:14,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 203/2199 [03:40<36:21,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 204/2199 [03:41<36:14,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 205/2199 [03:42<36:14,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 206/2199 [03:43<36:11,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 207/2199 [03:44<36:03,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|██▋                          | 208/2199 [03:45<35:59,  1.08s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 209/2199 [03:46<36:06,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 210/2199 [03:47<36:07,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 211/2199 [03:49<36:05,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 212/2199 [03:50<36:00,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 213/2199 [03:51<35:56,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 214/2199 [03:52<36:00,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 215/2199 [03:53<35:59,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 216/2199 [03:54<35:56,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 217/2199 [03:55<35:58,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▊                          | 218/2199 [03:56<35:58,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 219/2199 [03:57<35:56,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 220/2199 [03:58<35:58,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 221/2199 [03:59<35:57,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 222/2199 [04:01<35:53,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 223/2199 [04:02<35:57,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 224/2199 [04:03<35:56,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 225/2199 [04:04<35:48,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 226/2199 [04:05<35:48,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|██▉                          | 227/2199 [04:06<35:47,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|███                          | 228/2199 [04:07<35:47,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|███                          | 229/2199 [04:08<35:53,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|███                          | 230/2199 [04:09<35:47,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███                          | 231/2199 [04:10<35:45,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███                          | 232/2199 [04:11<35:40,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███                          | 233/2199 [04:13<35:32,  1.08s/it]\u001b[A\n",
      "Iteration:  11%|███                          | 234/2199 [04:14<35:36,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███                          | 235/2199 [04:15<35:37,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███                          | 236/2199 [04:16<35:32,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 237/2199 [04:17<35:33,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 238/2199 [04:18<35:36,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 239/2199 [04:19<35:34,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 240/2199 [04:20<35:35,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 241/2199 [04:21<35:34,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 242/2199 [04:22<35:27,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 243/2199 [04:23<35:30,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 244/2199 [04:24<35:31,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 245/2199 [04:26<35:25,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▏                         | 246/2199 [04:27<35:28,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▎                         | 247/2199 [04:28<35:27,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▎                         | 248/2199 [04:29<35:28,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▎                         | 249/2199 [04:30<35:23,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▎                         | 250/2199 [04:31<35:17,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▎                         | 251/2199 [04:32<35:21,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|███▎                         | 252/2199 [04:33<35:16,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▎                         | 253/2199 [04:34<35:17,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▎                         | 254/2199 [04:35<35:16,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▎                         | 255/2199 [04:36<35:16,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 256/2199 [04:38<35:12,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 257/2199 [04:39<35:09,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 258/2199 [04:40<35:11,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 259/2199 [04:41<35:14,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 260/2199 [04:42<35:12,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 261/2199 [04:43<35:10,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 262/2199 [04:44<35:03,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 263/2199 [04:45<35:08,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 264/2199 [04:46<35:11,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▍                         | 265/2199 [04:47<35:12,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 266/2199 [04:48<35:10,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 267/2199 [04:50<35:11,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 268/2199 [04:51<35:04,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 269/2199 [04:52<34:57,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 270/2199 [04:53<34:57,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 271/2199 [04:54<35:01,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 272/2199 [04:55<35:04,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 273/2199 [04:56<35:06,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|███▌                         | 274/2199 [04:57<35:10,  1.10s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 275/2199 [04:58<35:05,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 276/2199 [04:59<34:56,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 277/2199 [05:00<34:54,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 278/2199 [05:02<34:48,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 279/2199 [05:03<34:45,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 280/2199 [05:04<34:49,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 281/2199 [05:05<34:47,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 282/2199 [05:06<34:48,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 283/2199 [05:07<34:51,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▋                         | 284/2199 [05:08<34:52,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 285/2199 [05:09<34:45,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 286/2199 [05:10<34:46,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 287/2199 [05:11<34:45,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 288/2199 [05:12<34:50,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 289/2199 [05:14<34:47,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 290/2199 [05:15<34:44,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 291/2199 [05:16<34:43,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 292/2199 [05:17<34:40,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▊                         | 293/2199 [05:18<34:41,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▉                         | 294/2199 [05:19<34:35,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▉                         | 295/2199 [05:20<34:32,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|███▉                         | 296/2199 [05:21<34:27,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|███▉                         | 297/2199 [05:22<34:30,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|███▉                         | 298/2199 [05:23<34:26,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|███▉                         | 299/2199 [05:24<34:25,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|███▉                         | 300/2199 [05:26<34:28,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|███▉                         | 301/2199 [05:27<34:30,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|███▉                         | 302/2199 [05:28<34:26,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|███▉                         | 303/2199 [05:29<34:20,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 304/2199 [05:30<34:19,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 305/2199 [05:31<34:22,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 306/2199 [05:32<34:23,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 307/2199 [05:33<34:22,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 308/2199 [05:34<34:18,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 309/2199 [05:35<34:19,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 310/2199 [05:36<34:15,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 311/2199 [05:38<34:17,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████                         | 312/2199 [05:39<34:16,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████▏                        | 313/2199 [05:40<34:15,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████▏                        | 314/2199 [05:41<34:16,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████▏                        | 315/2199 [05:42<34:19,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████▏                        | 316/2199 [05:43<34:16,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████▏                        | 317/2199 [05:44<34:07,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|████▏                        | 318/2199 [05:45<34:05,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▏                        | 319/2199 [05:46<34:07,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▏                        | 320/2199 [05:47<34:10,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▏                        | 321/2199 [05:48<34:10,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▏                        | 322/2199 [05:50<34:09,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 323/2199 [05:51<34:07,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 324/2199 [05:52<34:02,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 325/2199 [05:53<34:04,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 326/2199 [05:54<34:05,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 327/2199 [05:55<34:05,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 328/2199 [05:56<34:05,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 329/2199 [05:57<34:01,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 330/2199 [05:58<33:54,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▎                        | 331/2199 [05:59<33:49,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 332/2199 [06:00<33:48,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 333/2199 [06:01<33:53,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 334/2199 [06:03<33:52,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 335/2199 [06:04<33:48,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 336/2199 [06:05<33:43,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 337/2199 [06:06<33:43,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 338/2199 [06:07<33:43,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 339/2199 [06:08<33:39,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|████▍                        | 340/2199 [06:09<33:47,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▍                        | 341/2199 [06:10<33:45,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 342/2199 [06:11<33:46,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 343/2199 [06:12<33:45,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 344/2199 [06:13<33:37,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 345/2199 [06:15<33:44,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 346/2199 [06:16<33:42,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 347/2199 [06:17<33:34,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 348/2199 [06:18<33:37,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 349/2199 [06:19<33:37,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▌                        | 350/2199 [06:20<33:33,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 351/2199 [06:21<33:25,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 352/2199 [06:22<33:18,  1.08s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 353/2199 [06:23<33:23,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 354/2199 [06:24<33:29,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 355/2199 [06:25<33:26,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 356/2199 [06:27<33:25,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 357/2199 [06:28<33:25,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 358/2199 [06:29<33:19,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 359/2199 [06:30<33:15,  1.08s/it]\u001b[A\n",
      "Iteration:  16%|████▋                        | 360/2199 [06:31<33:21,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▊                        | 361/2199 [06:32<33:18,  1.09s/it]\u001b[A\n",
      "Iteration:  16%|████▊                        | 362/2199 [06:33<33:20,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▊                        | 363/2199 [06:34<33:19,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▊                        | 364/2199 [06:35<33:20,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▊                        | 365/2199 [06:36<33:17,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▊                        | 366/2199 [06:37<33:14,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▊                        | 367/2199 [06:38<33:10,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▊                        | 368/2199 [06:40<33:06,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▊                        | 369/2199 [06:41<33:08,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▉                        | 370/2199 [06:42<33:08,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▉                        | 371/2199 [06:43<33:09,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▉                        | 372/2199 [06:44<33:04,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▉                        | 373/2199 [06:45<33:03,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▉                        | 374/2199 [06:46<33:08,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▉                        | 375/2199 [06:47<33:10,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▉                        | 376/2199 [06:48<33:10,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▉                        | 377/2199 [06:49<33:05,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▉                        | 378/2199 [06:50<33:02,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|████▉                        | 379/2199 [06:52<33:02,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|█████                        | 380/2199 [06:53<32:55,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|█████                        | 381/2199 [06:54<32:48,  1.08s/it]\u001b[A\n",
      "Iteration:  17%|█████                        | 382/2199 [06:55<32:46,  1.08s/it]\u001b[A\n",
      "Iteration:  17%|█████                        | 383/2199 [06:56<32:47,  1.08s/it]\u001b[A\n",
      "Iteration:  17%|█████                        | 384/2199 [06:57<32:52,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████                        | 385/2199 [06:58<32:53,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████                        | 386/2199 [06:59<32:52,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████                        | 387/2199 [07:00<32:46,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████                        | 388/2199 [07:01<32:50,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 389/2199 [07:02<32:51,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 390/2199 [07:03<32:50,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 391/2199 [07:05<32:45,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 392/2199 [07:06<32:46,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 393/2199 [07:07<32:40,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 394/2199 [07:08<32:45,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 395/2199 [07:09<32:41,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 396/2199 [07:10<32:39,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 397/2199 [07:11<32:37,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▏                       | 398/2199 [07:12<32:40,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 399/2199 [07:13<32:40,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 400/2199 [07:14<32:37,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 401/2199 [07:15<32:33,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 402/2199 [07:17<32:33,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 403/2199 [07:18<32:34,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 404/2199 [07:19<32:36,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 405/2199 [07:20<32:30,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█████▎                       | 406/2199 [07:21<32:29,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▎                       | 407/2199 [07:22<32:30,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 408/2199 [07:23<32:29,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 409/2199 [07:24<32:31,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 410/2199 [07:25<32:23,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 411/2199 [07:26<32:20,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 412/2199 [07:27<32:24,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 413/2199 [07:29<32:21,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 414/2199 [07:30<32:21,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 415/2199 [07:31<32:15,  1.08s/it]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 416/2199 [07:32<32:10,  1.08s/it]\u001b[A\n",
      "Iteration:  19%|█████▍                       | 417/2199 [07:33<32:14,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 418/2199 [07:34<32:14,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 419/2199 [07:35<32:12,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 420/2199 [07:36<32:14,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 421/2199 [07:37<32:09,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 422/2199 [07:38<32:06,  1.08s/it]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 423/2199 [07:39<32:06,  1.08s/it]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 424/2199 [07:40<32:05,  1.08s/it]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 425/2199 [07:42<32:06,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▌                       | 426/2199 [07:43<32:10,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▋                       | 427/2199 [07:44<32:14,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█████▋                       | 428/2199 [07:45<32:15,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▋                       | 429/2199 [07:46<32:12,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▋                       | 430/2199 [07:47<32:04,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▋                       | 431/2199 [07:48<32:01,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▋                       | 432/2199 [07:49<32:06,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▋                       | 433/2199 [07:50<32:07,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▋                       | 434/2199 [07:51<32:03,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▋                       | 435/2199 [07:52<32:07,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▋                       | 436/2199 [07:54<32:05,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 437/2199 [07:55<31:59,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 438/2199 [07:56<31:58,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 439/2199 [07:57<31:59,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 440/2199 [07:58<32:00,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 441/2199 [07:59<31:59,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 442/2199 [08:00<31:56,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 443/2199 [08:01<32:01,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 444/2199 [08:02<31:56,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▊                       | 445/2199 [08:03<31:51,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▉                       | 446/2199 [08:04<31:47,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▉                       | 447/2199 [08:06<31:49,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▉                       | 448/2199 [08:07<31:42,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▉                       | 449/2199 [08:08<31:49,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█████▉                       | 450/2199 [08:09<31:49,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|█████▉                       | 451/2199 [08:10<31:49,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|█████▉                       | 452/2199 [08:11<31:47,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|█████▉                       | 453/2199 [08:12<31:43,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|█████▉                       | 454/2199 [08:13<31:46,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████                       | 455/2199 [08:14<31:45,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████                       | 456/2199 [08:15<31:43,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████                       | 457/2199 [08:16<31:45,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████                       | 458/2199 [08:18<31:42,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████                       | 459/2199 [08:19<31:34,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████                       | 460/2199 [08:20<31:28,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████                       | 461/2199 [08:21<31:32,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████                       | 462/2199 [08:22<31:31,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████                       | 463/2199 [08:23<31:31,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████                       | 464/2199 [08:24<31:30,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 465/2199 [08:25<31:25,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 466/2199 [08:26<31:26,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 467/2199 [08:27<31:27,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 468/2199 [08:28<31:25,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 469/2199 [08:30<31:22,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 470/2199 [08:31<31:24,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 471/2199 [08:32<31:25,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██████▏                      | 472/2199 [08:33<31:24,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▏                      | 473/2199 [08:34<31:17,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 474/2199 [08:35<31:20,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 475/2199 [08:36<31:22,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 476/2199 [08:37<31:16,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 477/2199 [08:38<31:21,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 478/2199 [08:39<31:13,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 479/2199 [08:40<31:12,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 480/2199 [08:42<31:13,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 481/2199 [08:43<31:13,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 482/2199 [08:44<31:12,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▎                      | 483/2199 [08:45<31:16,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 484/2199 [08:46<31:15,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 485/2199 [08:47<31:13,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 486/2199 [08:48<31:12,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 487/2199 [08:49<31:07,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 488/2199 [08:50<31:07,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 489/2199 [08:51<31:08,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 490/2199 [08:52<31:01,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 491/2199 [08:54<31:04,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▍                      | 492/2199 [08:55<30:58,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▌                      | 493/2199 [08:56<30:55,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██████▌                      | 494/2199 [08:57<30:57,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██████▌                      | 495/2199 [08:58<30:51,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██████▌                      | 496/2199 [08:59<30:52,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██████▌                      | 497/2199 [09:00<30:57,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██████▌                      | 498/2199 [09:01<30:55,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██████▌                      | 499/2199 [09:02<30:55,  1.09s/it]\u001b[A03/11/2020 09:42:34 - INFO - transformers.configuration_utils -   Configuration saved in ./bert_model/newsbert_owntokenizer_vsize5w_woblocksize/checkpoint-500/config.json\n",
      "03/11/2020 09:42:34 - INFO - transformers.modeling_utils -   Model weights saved in ./bert_model/newsbert_owntokenizer_vsize5w_woblocksize/checkpoint-500/pytorch_model.bin\n",
      "03/11/2020 09:42:34 - INFO - __main__ -   Saving model checkpoint to ./bert_model/newsbert_owntokenizer_vsize5w_woblocksize/checkpoint-500\n",
      "03/11/2020 09:42:35 - INFO - __main__ -   Saving optimizer and scheduler states to ./bert_model/newsbert_owntokenizer_vsize5w_woblocksize/checkpoint-500\n",
      "\n",
      "Iteration:  23%|██████▌                      | 500/2199 [09:05<41:57,  1.48s/it]\u001b[A\n",
      "Iteration:  23%|██████▌                      | 501/2199 [09:06<38:41,  1.37s/it]\u001b[A\n",
      "Iteration:  23%|██████▌                      | 502/2199 [09:07<36:14,  1.28s/it]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 503/2199 [09:08<34:36,  1.22s/it]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 504/2199 [09:09<33:23,  1.18s/it]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 505/2199 [09:10<32:41,  1.16s/it]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 506/2199 [09:11<32:03,  1.14s/it]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 507/2199 [09:12<31:35,  1.12s/it]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 508/2199 [09:13<31:16,  1.11s/it]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 509/2199 [09:14<30:59,  1.10s/it]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 510/2199 [09:16<30:49,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██████▋                      | 511/2199 [09:17<30:46,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██████▊                      | 512/2199 [09:18<30:44,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██████▊                      | 513/2199 [09:19<30:41,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██████▊                      | 514/2199 [09:20<30:39,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██████▊                      | 515/2199 [09:21<30:34,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██████▊                      | 516/2199 [09:22<30:31,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|██████▊                      | 517/2199 [09:23<30:38,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|██████▊                      | 518/2199 [09:24<30:34,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|██████▊                      | 519/2199 [09:25<30:39,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|██████▊                      | 520/2199 [09:26<30:29,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|██████▊                      | 521/2199 [09:27<30:29,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 522/2199 [09:29<30:30,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 523/2199 [09:30<30:28,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 524/2199 [09:31<30:26,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 525/2199 [09:32<30:23,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 526/2199 [09:33<30:18,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 527/2199 [09:34<30:21,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 528/2199 [09:35<30:22,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 529/2199 [09:36<30:19,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|██████▉                      | 530/2199 [09:37<30:21,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 531/2199 [09:38<30:23,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 532/2199 [09:40<30:24,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 533/2199 [09:41<30:20,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 534/2199 [09:42<30:17,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 535/2199 [09:43<30:14,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 536/2199 [09:44<30:08,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 537/2199 [09:45<30:05,  1.09s/it]\u001b[A\n",
      "Iteration:  24%|███████                      | 538/2199 [09:46<30:03,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████                      | 539/2199 [09:47<30:07,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████                      | 540/2199 [09:48<30:00,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 541/2199 [09:49<29:56,  1.08s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 542/2199 [09:50<30:01,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 543/2199 [09:51<30:02,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 544/2199 [09:53<30:00,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 545/2199 [09:54<30:04,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 546/2199 [09:55<30:01,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 547/2199 [09:56<29:58,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 548/2199 [09:57<29:58,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▏                     | 549/2199 [09:58<29:57,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 550/2199 [09:59<29:55,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 551/2199 [10:00<30:01,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 552/2199 [10:01<29:54,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 553/2199 [10:02<29:53,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 554/2199 [10:03<29:51,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 555/2199 [10:05<29:48,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 556/2199 [10:06<29:47,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 557/2199 [10:07<29:44,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 558/2199 [10:08<29:44,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▎                     | 559/2199 [10:09<29:41,  1.09s/it]\u001b[A\n",
      "Iteration:  25%|███████▍                     | 560/2199 [10:10<29:44,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▍                     | 561/2199 [10:11<29:47,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▍                     | 562/2199 [10:12<29:42,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▍                     | 563/2199 [10:13<29:41,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▍                     | 564/2199 [10:14<29:39,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▍                     | 565/2199 [10:15<29:41,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▍                     | 566/2199 [10:17<29:37,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▍                     | 567/2199 [10:18<29:38,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▍                     | 568/2199 [10:19<29:40,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▌                     | 569/2199 [10:20<29:41,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▌                     | 570/2199 [10:21<29:36,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▌                     | 571/2199 [10:22<29:35,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▌                     | 572/2199 [10:23<29:34,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▌                     | 573/2199 [10:24<29:33,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▌                     | 574/2199 [10:25<29:31,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▌                     | 575/2199 [10:26<29:26,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▌                     | 576/2199 [10:27<29:27,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▌                     | 577/2199 [10:29<29:25,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▌                     | 578/2199 [10:30<29:21,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▋                     | 579/2199 [10:31<29:18,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▋                     | 580/2199 [10:32<29:21,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▋                     | 581/2199 [10:33<29:23,  1.09s/it]\u001b[A\n",
      "Iteration:  26%|███████▋                     | 582/2199 [10:34<29:24,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▋                     | 583/2199 [10:35<29:24,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▋                     | 584/2199 [10:36<29:21,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▋                     | 585/2199 [10:37<29:14,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▋                     | 586/2199 [10:38<29:14,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▋                     | 587/2199 [10:39<29:14,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▊                     | 588/2199 [10:40<29:16,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▊                     | 589/2199 [10:42<29:19,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▊                     | 590/2199 [10:43<29:15,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▊                     | 591/2199 [10:44<29:09,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▊                     | 592/2199 [10:45<29:07,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▊                     | 593/2199 [10:46<29:06,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▊                     | 594/2199 [10:47<29:07,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▊                     | 595/2199 [10:48<29:04,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▊                     | 596/2199 [10:49<29:05,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▊                     | 597/2199 [10:50<29:06,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▉                     | 598/2199 [10:51<29:01,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▉                     | 599/2199 [10:52<29:02,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▉                     | 600/2199 [10:54<29:00,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▉                     | 601/2199 [10:55<28:57,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▉                     | 602/2199 [10:56<28:58,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▉                     | 603/2199 [10:57<28:54,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|███████▉                     | 604/2199 [10:58<28:55,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|███████▉                     | 605/2199 [10:59<28:57,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|███████▉                     | 606/2199 [11:00<28:52,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████                     | 607/2199 [11:01<28:51,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████                     | 608/2199 [11:02<28:50,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████                     | 609/2199 [11:03<28:47,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████                     | 610/2199 [11:04<28:47,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████                     | 611/2199 [11:06<28:43,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████                     | 612/2199 [11:07<28:39,  1.08s/it]\u001b[A\n",
      "Iteration:  28%|████████                     | 613/2199 [11:08<28:39,  1.08s/it]\u001b[A\n",
      "Iteration:  28%|████████                     | 614/2199 [11:09<28:38,  1.08s/it]\u001b[A\n",
      "Iteration:  28%|████████                     | 615/2199 [11:10<28:39,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████                     | 616/2199 [11:11<28:42,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████▏                    | 617/2199 [11:12<28:43,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████▏                    | 618/2199 [11:13<28:40,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████▏                    | 619/2199 [11:14<28:44,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████▏                    | 620/2199 [11:15<28:41,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████▏                    | 621/2199 [11:16<28:38,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████▏                    | 622/2199 [11:17<28:38,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████▏                    | 623/2199 [11:19<28:37,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████▏                    | 624/2199 [11:20<28:39,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████▏                    | 625/2199 [11:21<28:39,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|████████▎                    | 626/2199 [11:22<28:37,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▎                    | 627/2199 [11:23<28:41,  1.10s/it]\u001b[A\n",
      "Iteration:  29%|████████▎                    | 628/2199 [11:24<28:38,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▎                    | 629/2199 [11:25<28:32,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▎                    | 630/2199 [11:26<28:31,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▎                    | 631/2199 [11:27<28:30,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▎                    | 632/2199 [11:28<28:32,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▎                    | 633/2199 [11:29<28:31,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▎                    | 634/2199 [11:31<28:24,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▎                    | 635/2199 [11:32<28:21,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▍                    | 636/2199 [11:33<28:18,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▍                    | 637/2199 [11:34<28:15,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▍                    | 638/2199 [11:35<28:16,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▍                    | 639/2199 [11:36<28:16,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▍                    | 640/2199 [11:37<28:15,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▍                    | 641/2199 [11:38<28:14,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▍                    | 642/2199 [11:39<28:17,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▍                    | 643/2199 [11:40<28:13,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▍                    | 644/2199 [11:41<28:08,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▌                    | 645/2199 [11:43<28:14,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▌                    | 646/2199 [11:44<28:16,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▌                    | 647/2199 [11:45<28:19,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|████████▌                    | 648/2199 [11:46<28:11,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▌                    | 649/2199 [11:47<28:08,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▌                    | 650/2199 [11:48<28:09,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▌                    | 651/2199 [11:49<28:09,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▌                    | 652/2199 [11:50<28:09,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▌                    | 653/2199 [11:51<28:10,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▌                    | 654/2199 [11:52<28:08,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▋                    | 655/2199 [11:53<28:07,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▋                    | 656/2199 [11:55<28:08,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▋                    | 657/2199 [11:56<28:04,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▋                    | 658/2199 [11:57<28:03,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▋                    | 659/2199 [11:58<28:02,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▋                    | 660/2199 [11:59<27:59,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▋                    | 661/2199 [12:00<27:56,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▋                    | 662/2199 [12:01<27:53,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▋                    | 663/2199 [12:02<27:52,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▊                    | 664/2199 [12:03<27:54,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▊                    | 665/2199 [12:04<27:49,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▊                    | 666/2199 [12:05<27:52,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▊                    | 667/2199 [12:07<27:51,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▊                    | 668/2199 [12:08<27:47,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▊                    | 669/2199 [12:09<27:42,  1.09s/it]\u001b[A\n",
      "Iteration:  30%|████████▊                    | 670/2199 [12:10<27:40,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|████████▊                    | 671/2199 [12:11<27:40,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|████████▊                    | 672/2199 [12:12<27:40,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|████████▉                    | 673/2199 [12:13<27:45,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|████████▉                    | 674/2199 [12:14<27:45,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|████████▉                    | 675/2199 [12:15<27:42,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|████████▉                    | 676/2199 [12:16<27:44,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|████████▉                    | 677/2199 [12:17<27:41,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|████████▉                    | 678/2199 [12:19<27:39,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|████████▉                    | 679/2199 [12:20<27:39,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|████████▉                    | 680/2199 [12:21<27:34,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|████████▉                    | 681/2199 [12:22<27:35,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|████████▉                    | 682/2199 [12:23<27:34,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|█████████                    | 683/2199 [12:24<27:30,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|█████████                    | 684/2199 [12:25<27:25,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|█████████                    | 685/2199 [12:26<27:27,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|█████████                    | 686/2199 [12:27<27:29,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|█████████                    | 687/2199 [12:28<27:28,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|█████████                    | 688/2199 [12:29<27:27,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|█████████                    | 689/2199 [12:31<27:26,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|█████████                    | 690/2199 [12:32<27:25,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|█████████                    | 691/2199 [12:33<27:19,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|█████████▏                   | 692/2199 [12:34<27:19,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▏                   | 693/2199 [12:35<27:22,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▏                   | 694/2199 [12:36<27:19,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▏                   | 695/2199 [12:37<27:17,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▏                   | 696/2199 [12:38<27:14,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▏                   | 697/2199 [12:39<27:11,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▏                   | 698/2199 [12:40<27:14,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▏                   | 699/2199 [12:41<27:14,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▏                   | 700/2199 [12:43<27:14,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▏                   | 701/2199 [12:44<27:17,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▎                   | 702/2199 [12:45<27:14,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▎                   | 703/2199 [12:46<27:10,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▎                   | 704/2199 [12:47<27:07,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▎                   | 705/2199 [12:48<27:08,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▎                   | 706/2199 [12:49<27:07,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▎                   | 707/2199 [12:50<27:07,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▎                   | 708/2199 [12:51<27:03,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▎                   | 709/2199 [12:52<27:05,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▎                   | 710/2199 [12:53<27:05,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▍                   | 711/2199 [12:55<27:01,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▍                   | 712/2199 [12:56<27:04,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▍                   | 713/2199 [12:57<27:04,  1.09s/it]\u001b[A\n",
      "Iteration:  32%|█████████▍                   | 714/2199 [12:58<26:57,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▍                   | 715/2199 [12:59<26:54,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▍                   | 716/2199 [13:00<26:54,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▍                   | 717/2199 [13:01<26:46,  1.08s/it]\u001b[A\n",
      "Iteration:  33%|█████████▍                   | 718/2199 [13:02<26:50,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▍                   | 719/2199 [13:03<26:53,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▍                   | 720/2199 [13:04<26:50,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▌                   | 721/2199 [13:05<26:54,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▌                   | 722/2199 [13:06<26:55,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▌                   | 723/2199 [13:08<26:52,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▌                   | 724/2199 [13:09<26:49,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▌                   | 725/2199 [13:10<26:48,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▌                   | 726/2199 [13:11<26:51,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▌                   | 727/2199 [13:12<26:48,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▌                   | 728/2199 [13:13<26:45,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▌                   | 729/2199 [13:14<26:47,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▋                   | 730/2199 [13:15<26:41,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▋                   | 731/2199 [13:16<26:36,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▋                   | 732/2199 [13:17<26:38,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▋                   | 733/2199 [13:18<26:40,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▋                   | 734/2199 [13:20<26:40,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▋                   | 735/2199 [13:21<26:38,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|█████████▋                   | 736/2199 [13:22<26:37,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▋                   | 737/2199 [13:23<26:35,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▋                   | 738/2199 [13:24<26:38,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▋                   | 739/2199 [13:25<26:33,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▊                   | 740/2199 [13:26<26:33,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▊                   | 741/2199 [13:27<26:27,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▊                   | 742/2199 [13:28<26:28,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▊                   | 743/2199 [13:29<26:25,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▊                   | 744/2199 [13:30<26:25,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▊                   | 745/2199 [13:32<26:26,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▊                   | 746/2199 [13:33<26:27,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▊                   | 747/2199 [13:34<26:24,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▊                   | 748/2199 [13:35<26:25,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▉                   | 749/2199 [13:36<26:19,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▉                   | 750/2199 [13:37<26:17,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▉                   | 751/2199 [13:38<26:12,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▉                   | 752/2199 [13:39<26:15,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▉                   | 753/2199 [13:40<26:11,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▉                   | 754/2199 [13:41<26:12,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▉                   | 755/2199 [13:42<26:14,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▉                   | 756/2199 [13:44<26:15,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▉                   | 757/2199 [13:45<26:15,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|█████████▉                   | 758/2199 [13:46<26:16,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████                   | 759/2199 [13:47<26:08,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████                   | 760/2199 [13:48<26:04,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████                   | 761/2199 [13:49<26:01,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████                   | 762/2199 [13:50<26:02,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████                   | 763/2199 [13:51<26:06,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████                   | 764/2199 [13:52<26:05,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████                   | 765/2199 [13:53<26:05,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████                   | 766/2199 [13:54<26:07,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████                   | 767/2199 [13:56<26:06,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████▏                  | 768/2199 [13:57<26:03,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████▏                  | 769/2199 [13:58<26:02,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████▏                  | 770/2199 [13:59<25:58,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████▏                  | 771/2199 [14:00<25:54,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████▏                  | 772/2199 [14:01<25:53,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████▏                  | 773/2199 [14:02<25:52,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████▏                  | 774/2199 [14:03<25:48,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████▏                  | 775/2199 [14:04<25:50,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████▏                  | 776/2199 [14:05<25:51,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████▏                  | 777/2199 [14:06<25:51,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████▎                  | 778/2199 [14:08<25:47,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████▎                  | 779/2199 [14:09<25:43,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|██████████▎                  | 780/2199 [14:10<25:44,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▎                  | 781/2199 [14:11<25:44,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▎                  | 782/2199 [14:12<25:45,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▎                  | 783/2199 [14:13<25:44,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▎                  | 784/2199 [14:14<25:39,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▎                  | 785/2199 [14:15<25:40,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▎                  | 786/2199 [14:16<25:40,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▍                  | 787/2199 [14:17<25:38,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▍                  | 788/2199 [14:18<25:39,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▍                  | 789/2199 [14:20<25:34,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▍                  | 790/2199 [14:21<25:33,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▍                  | 791/2199 [14:22<25:33,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▍                  | 792/2199 [14:23<25:31,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▍                  | 793/2199 [14:24<25:30,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▍                  | 794/2199 [14:25<25:31,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▍                  | 795/2199 [14:26<25:31,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▍                  | 796/2199 [14:27<25:33,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▌                  | 797/2199 [14:28<25:29,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▌                  | 798/2199 [14:29<25:29,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▌                  | 799/2199 [14:30<25:28,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▌                  | 800/2199 [14:32<25:28,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▌                  | 801/2199 [14:33<25:25,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|██████████▌                  | 802/2199 [14:34<25:22,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▌                  | 803/2199 [14:35<25:24,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▌                  | 804/2199 [14:36<25:22,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▌                  | 805/2199 [14:37<25:19,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▋                  | 806/2199 [14:38<25:19,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▋                  | 807/2199 [14:39<25:15,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▋                  | 808/2199 [14:40<25:17,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▋                  | 809/2199 [14:41<25:16,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▋                  | 810/2199 [14:42<25:12,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▋                  | 811/2199 [14:44<25:16,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▋                  | 812/2199 [14:45<25:09,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▋                  | 813/2199 [14:46<25:10,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▋                  | 814/2199 [14:47<25:13,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▋                  | 815/2199 [14:48<25:09,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▊                  | 816/2199 [14:49<25:09,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▊                  | 817/2199 [14:50<25:08,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▊                  | 818/2199 [14:51<25:08,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▊                  | 819/2199 [14:52<25:08,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▊                  | 820/2199 [14:53<25:06,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▊                  | 821/2199 [14:54<25:05,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▊                  | 822/2199 [14:56<25:04,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▊                  | 823/2199 [14:57<24:56,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|██████████▊                  | 824/2199 [14:58<24:57,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|██████████▉                  | 825/2199 [14:59<24:59,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|██████████▉                  | 826/2199 [15:00<24:54,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|██████████▉                  | 827/2199 [15:01<24:50,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|██████████▉                  | 828/2199 [15:02<24:51,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|██████████▉                  | 829/2199 [15:03<24:51,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|██████████▉                  | 830/2199 [15:04<24:47,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|██████████▉                  | 831/2199 [15:05<24:52,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|██████████▉                  | 832/2199 [15:06<24:48,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|██████████▉                  | 833/2199 [15:08<24:46,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|██████████▉                  | 834/2199 [15:09<24:47,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|███████████                  | 835/2199 [15:10<24:45,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|███████████                  | 836/2199 [15:11<24:47,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|███████████                  | 837/2199 [15:12<24:46,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|███████████                  | 838/2199 [15:13<24:46,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|███████████                  | 839/2199 [15:14<24:47,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|███████████                  | 840/2199 [15:15<24:41,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|███████████                  | 841/2199 [15:16<24:40,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|███████████                  | 842/2199 [15:17<24:37,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|███████████                  | 843/2199 [15:18<24:37,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|███████████▏                 | 844/2199 [15:20<24:37,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|███████████▏                 | 845/2199 [15:21<24:33,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|███████████▏                 | 846/2199 [15:22<24:30,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▏                 | 847/2199 [15:23<24:31,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▏                 | 848/2199 [15:24<24:33,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▏                 | 849/2199 [15:25<24:33,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▏                 | 850/2199 [15:26<24:32,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▏                 | 851/2199 [15:27<24:30,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▏                 | 852/2199 [15:28<24:30,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▏                 | 853/2199 [15:29<24:29,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▎                 | 854/2199 [15:30<24:29,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▎                 | 855/2199 [15:32<24:23,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▎                 | 856/2199 [15:33<24:18,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▎                 | 857/2199 [15:34<24:19,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▎                 | 858/2199 [15:35<24:15,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▎                 | 859/2199 [15:36<24:20,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▎                 | 860/2199 [15:37<24:20,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▎                 | 861/2199 [15:38<24:17,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▎                 | 862/2199 [15:39<24:13,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▍                 | 863/2199 [15:40<24:15,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▍                 | 864/2199 [15:41<24:15,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▍                 | 865/2199 [15:42<24:16,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▍                 | 866/2199 [15:43<24:15,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▍                 | 867/2199 [15:45<24:17,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███████████▍                 | 868/2199 [15:46<24:13,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▍                 | 869/2199 [15:47<24:12,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▍                 | 870/2199 [15:48<24:08,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▍                 | 871/2199 [15:49<24:08,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▍                 | 872/2199 [15:50<24:09,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▌                 | 873/2199 [15:51<24:08,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▌                 | 874/2199 [15:52<24:06,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▌                 | 875/2199 [15:53<24:02,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▌                 | 876/2199 [15:54<24:02,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▌                 | 877/2199 [15:55<24:01,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▌                 | 878/2199 [15:57<23:56,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▌                 | 879/2199 [15:58<23:54,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▌                 | 880/2199 [15:59<23:51,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▌                 | 881/2199 [16:00<23:49,  1.08s/it]\u001b[A\n",
      "Iteration:  40%|███████████▋                 | 882/2199 [16:01<23:47,  1.08s/it]\u001b[A\n",
      "Iteration:  40%|███████████▋                 | 883/2199 [16:02<23:52,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▋                 | 884/2199 [16:03<23:51,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▋                 | 885/2199 [16:04<23:50,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▋                 | 886/2199 [16:05<23:48,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▋                 | 887/2199 [16:06<23:47,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▋                 | 888/2199 [16:07<23:46,  1.09s/it]\u001b[A\n",
      "Iteration:  40%|███████████▋                 | 889/2199 [16:09<23:39,  1.08s/it]\u001b[A\n",
      "Iteration:  40%|███████████▋                 | 890/2199 [16:10<23:41,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▊                 | 891/2199 [16:11<23:47,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▊                 | 892/2199 [16:12<23:48,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▊                 | 893/2199 [16:13<23:49,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▊                 | 894/2199 [16:14<23:44,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▊                 | 895/2199 [16:15<23:44,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▊                 | 896/2199 [16:16<23:40,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▊                 | 897/2199 [16:17<23:40,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▊                 | 898/2199 [16:18<23:41,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▊                 | 899/2199 [16:19<23:40,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▊                 | 900/2199 [16:21<23:40,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▉                 | 901/2199 [16:22<23:43,  1.10s/it]\u001b[A\n",
      "Iteration:  41%|███████████▉                 | 902/2199 [16:23<23:36,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▉                 | 903/2199 [16:24<23:32,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▉                 | 904/2199 [16:25<23:31,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▉                 | 905/2199 [16:26<23:27,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▉                 | 906/2199 [16:27<23:23,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▉                 | 907/2199 [16:28<23:23,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▉                 | 908/2199 [16:29<23:22,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|███████████▉                 | 909/2199 [16:30<23:21,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|████████████                 | 910/2199 [16:31<23:19,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|████████████                 | 911/2199 [16:33<23:22,  1.09s/it]\u001b[A\n",
      "Iteration:  41%|████████████                 | 912/2199 [16:34<23:22,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████                 | 913/2199 [16:35<23:20,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████                 | 914/2199 [16:36<23:16,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████                 | 915/2199 [16:37<23:13,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████                 | 916/2199 [16:38<23:14,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████                 | 917/2199 [16:39<23:14,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████                 | 918/2199 [16:40<23:16,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████                 | 919/2199 [16:41<23:16,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████▏                | 920/2199 [16:42<23:17,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████▏                | 921/2199 [16:43<23:13,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████▏                | 922/2199 [16:44<23:11,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████▏                | 923/2199 [16:46<23:11,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████▏                | 924/2199 [16:47<23:14,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████▏                | 925/2199 [16:48<23:12,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████▏                | 926/2199 [16:49<23:10,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████▏                | 927/2199 [16:50<23:08,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████▏                | 928/2199 [16:51<23:05,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████▎                | 929/2199 [16:52<23:03,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████▎                | 930/2199 [16:53<23:01,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████▎                | 931/2199 [16:54<22:55,  1.08s/it]\u001b[A\n",
      "Iteration:  42%|████████████▎                | 932/2199 [16:55<22:55,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████▎                | 933/2199 [16:56<22:59,  1.09s/it]\u001b[A\n",
      "Iteration:  42%|████████████▎                | 934/2199 [16:58<22:58,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▎                | 935/2199 [16:59<22:57,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▎                | 936/2199 [17:00<22:56,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▎                | 937/2199 [17:01<22:55,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▎                | 938/2199 [17:02<22:57,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▍                | 939/2199 [17:03<22:56,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▍                | 940/2199 [17:04<22:54,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▍                | 941/2199 [17:05<22:54,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▍                | 942/2199 [17:06<22:54,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▍                | 943/2199 [17:07<22:52,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▍                | 944/2199 [17:09<22:52,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▍                | 945/2199 [17:10<22:50,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▍                | 946/2199 [17:11<22:46,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▍                | 947/2199 [17:12<22:50,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▌                | 948/2199 [17:13<22:46,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▌                | 949/2199 [17:14<22:49,  1.10s/it]\u001b[A\n",
      "Iteration:  43%|████████████▌                | 950/2199 [17:15<22:47,  1.10s/it]\u001b[A\n",
      "Iteration:  43%|████████████▌                | 951/2199 [17:16<22:46,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▌                | 952/2199 [17:17<22:44,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▌                | 953/2199 [17:18<22:38,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▌                | 954/2199 [17:19<22:36,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▌                | 955/2199 [17:21<22:37,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████████████▌                | 956/2199 [17:22<22:35,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▌                | 957/2199 [17:23<22:36,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▋                | 958/2199 [17:24<22:35,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▋                | 959/2199 [17:25<22:33,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▋                | 960/2199 [17:26<22:29,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▋                | 961/2199 [17:27<22:26,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▋                | 962/2199 [17:28<22:24,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▋                | 963/2199 [17:29<22:25,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▋                | 964/2199 [17:30<22:27,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▋                | 965/2199 [17:31<22:26,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▋                | 966/2199 [17:33<22:26,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▊                | 967/2199 [17:34<22:23,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▊                | 968/2199 [17:35<22:22,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▊                | 969/2199 [17:36<22:17,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▊                | 970/2199 [17:37<22:18,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▊                | 971/2199 [17:38<22:17,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▊                | 972/2199 [17:39<22:19,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▊                | 973/2199 [17:40<22:15,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▊                | 974/2199 [17:41<22:14,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▊                | 975/2199 [17:42<22:14,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▊                | 976/2199 [17:43<22:12,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▉                | 977/2199 [17:44<22:11,  1.09s/it]\u001b[A\n",
      "Iteration:  44%|████████████▉                | 978/2199 [17:46<22:11,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|████████████▉                | 979/2199 [17:47<22:09,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|████████████▉                | 980/2199 [17:48<22:09,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|████████████▉                | 981/2199 [17:49<22:08,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|████████████▉                | 982/2199 [17:50<22:03,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|████████████▉                | 983/2199 [17:51<22:07,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|████████████▉                | 984/2199 [17:52<22:03,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|████████████▉                | 985/2199 [17:53<22:02,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|█████████████                | 986/2199 [17:54<22:03,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|█████████████                | 987/2199 [17:55<22:02,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|█████████████                | 988/2199 [17:56<22:01,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|█████████████                | 989/2199 [17:58<22:00,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|█████████████                | 990/2199 [17:59<21:58,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|█████████████                | 991/2199 [18:00<21:57,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|█████████████                | 992/2199 [18:01<21:56,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|█████████████                | 993/2199 [18:02<21:53,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|█████████████                | 994/2199 [18:03<21:53,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|█████████████                | 995/2199 [18:04<21:49,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|█████████████▏               | 996/2199 [18:05<21:50,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|█████████████▏               | 997/2199 [18:06<21:53,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|█████████████▏               | 998/2199 [18:07<21:52,  1.09s/it]\u001b[A\n",
      "Iteration:  45%|█████████████▏               | 999/2199 [18:08<21:50,  1.09s/it]\u001b[A03/11/2020 09:51:40 - INFO - transformers.configuration_utils -   Configuration saved in ./bert_model/newsbert_owntokenizer_vsize5w_woblocksize/checkpoint-1000/config.json\n",
      "03/11/2020 09:51:41 - INFO - transformers.modeling_utils -   Model weights saved in ./bert_model/newsbert_owntokenizer_vsize5w_woblocksize/checkpoint-1000/pytorch_model.bin\n",
      "03/11/2020 09:51:41 - INFO - __main__ -   Saving model checkpoint to ./bert_model/newsbert_owntokenizer_vsize5w_woblocksize/checkpoint-1000\n",
      "03/11/2020 09:51:41 - INFO - __main__ -   Saving optimizer and scheduler states to ./bert_model/newsbert_owntokenizer_vsize5w_woblocksize/checkpoint-1000\n",
      "\n",
      "Iteration:  45%|████████████▋               | 1000/2199 [18:11<29:24,  1.47s/it]\u001b[A\n",
      "Iteration:  46%|████████████▋               | 1001/2199 [18:12<27:03,  1.36s/it]\u001b[A\n",
      "Iteration:  46%|████████████▊               | 1002/2199 [18:13<25:25,  1.27s/it]\u001b[A\n",
      "Iteration:  46%|████████████▊               | 1003/2199 [18:14<24:17,  1.22s/it]\u001b[A\n",
      "Iteration:  46%|████████████▊               | 1004/2199 [18:15<23:28,  1.18s/it]\u001b[A\n",
      "Iteration:  46%|████████████▊               | 1005/2199 [18:16<22:59,  1.16s/it]\u001b[A\n",
      "Iteration:  46%|████████████▊               | 1006/2199 [18:17<22:32,  1.13s/it]\u001b[A\n",
      "Iteration:  46%|████████████▊               | 1007/2199 [18:18<22:15,  1.12s/it]\u001b[A\n",
      "Iteration:  46%|████████████▊               | 1008/2199 [18:20<22:03,  1.11s/it]\u001b[A\n",
      "Iteration:  46%|████████████▊               | 1009/2199 [18:21<21:51,  1.10s/it]\u001b[A\n",
      "Iteration:  46%|████████████▊               | 1010/2199 [18:22<21:40,  1.09s/it]\u001b[A\n",
      "Iteration:  46%|████████████▊               | 1011/2199 [18:23<21:41,  1.10s/it]\u001b[A\n",
      "Iteration:  46%|████████████▉               | 1012/2199 [18:24<21:39,  1.09s/it]\u001b[A\n",
      "Iteration:  46%|████████████▉               | 1013/2199 [18:25<21:37,  1.09s/it]\u001b[A\n",
      "Iteration:  46%|████████████▉               | 1014/2199 [18:26<21:39,  1.10s/it]\u001b[A\n",
      "Iteration:  46%|████████████▉               | 1015/2199 [18:27<21:35,  1.09s/it]\u001b[A\n",
      "Iteration:  46%|████████████▉               | 1016/2199 [18:28<21:33,  1.09s/it]\u001b[A\n",
      "Iteration:  46%|████████████▉               | 1017/2199 [18:29<21:31,  1.09s/it]\u001b[A\n",
      "Iteration:  46%|████████████▉               | 1018/2199 [18:30<21:29,  1.09s/it]\u001b[A\n",
      "Iteration:  46%|████████████▉               | 1019/2199 [18:32<21:28,  1.09s/it]\u001b[A\n",
      "Iteration:  46%|████████████▉               | 1020/2199 [18:33<21:28,  1.09s/it]\u001b[A\n",
      "Iteration:  46%|█████████████               | 1021/2199 [18:34<21:26,  1.09s/it]\u001b[A\n",
      "Iteration:  46%|█████████████               | 1022/2199 [18:35<21:28,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████               | 1023/2199 [18:36<21:25,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████               | 1024/2199 [18:37<21:24,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████               | 1025/2199 [18:38<21:20,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████               | 1026/2199 [18:39<21:18,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████               | 1027/2199 [18:40<21:14,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████               | 1028/2199 [18:41<21:14,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████               | 1029/2199 [18:42<21:11,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████               | 1030/2199 [18:44<21:08,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████▏              | 1031/2199 [18:45<21:09,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████▏              | 1032/2199 [18:46<21:10,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████▏              | 1033/2199 [18:47<21:06,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████▏              | 1034/2199 [18:48<21:09,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████▏              | 1035/2199 [18:49<21:11,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████▏              | 1036/2199 [18:50<21:09,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████▏              | 1037/2199 [18:51<21:10,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████▏              | 1038/2199 [18:52<21:08,  1.09s/it]\u001b[A\n",
      "Iteration:  47%|█████████████▏              | 1039/2199 [18:53<21:06,  1.09s/it]\u001b[A^C\n",
      "Traceback (most recent call last):\n",
      "  File \"transformers/examples/run_language_modeling.py\", line 798, in <module>\n",
      "    main()\n",
      "  File \"transformers/examples/run_language_modeling.py\", line 748, in main\n",
      "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
      "  File \"transformers/examples/run_language_modeling.py\", line 371, in train\n",
      "    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\", line 38, in clip_grad_norm_\n",
      "    p.grad.data.mul_(clip_coef)\n",
      "KeyboardInterrupt\n",
      "\n",
      "Epoch:   0%|                                              | 0/1 [18:55<?, ?it/s]\n",
      "Iteration:  47%|█████████████▏              | 1039/2199 [18:55<21:07,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "#!cd /content/transformers\n",
    "!python transformers/examples/run_language_modeling.py \\\n",
    "    --output_dir=./bert_model/newsbert_owntokenizer_vsize5w_woblocksize \\\n",
    "    --model_type=bert \\\n",
    "    --do_train \\\n",
    "    --train_data_file=\"trainfile\" \\\n",
    "    --do_eval \\\n",
    "#    --model_name_or_path=bert_model/newsbert_owntokenizer_vsize5w_woblocksize/checkpoint-1000 \\\n",
    "    --tokenizer_name='tokenizer/newstokenizer1_vsize50000/' \\\n",
    "    --eval_data_file=\"evalfile\" \\\n",
    "    --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 用trader標記過的資料看結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_pickle(path):\n",
    "    f = open(path, 'rb')\n",
    "    df = pickle.load(f)\n",
    "    f.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled = load_pickle('newsapi-dataset/IF_NewsDataSet_Labeled.pkl')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切train val test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled.drop_duplicates(subset =\"content\", keep = \"first\", inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled.dropna(subset=['content'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled.loc[df_labeled['scores'] == -1, 'scores'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    836\n",
       "1    112\n",
       "Name: scores, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled['scores'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled_pos = df_labeled[df_labeled['scores'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled_neg = df_labeled[df_labeled['scores'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pos, df_train_neg = df_labeled_pos.sample(90), df_labeled_neg.sample(90)\n",
    "df_valtest_pos, df_valtest_neg = pd.concat([df_labeled_pos, df_train_pos]).drop_duplicates(keep=False), \\\n",
    "    pd.concat([df_labeled_neg, df_train_neg]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train_pos, df_train_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_pos, df_val_neg = df_valtest_pos.sample(frac = 0.5), df_valtest_neg.sample(frac = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pos, df_test_neg = pd.concat([df_val_pos, df_valtest_pos]).drop_duplicates(keep=False), \\\n",
    "    pd.concat([df_val_neg, df_valtest_neg]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.concat([df_val_pos, df_val_neg])\n",
    "df_test = pd.concat([df_test_pos, df_test_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size:  180\n",
      "Validation Set Size:  384\n",
      "Testing Set Size:  384\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set Size: \", len(df_train))\n",
    "print(\"Validation Set Size: \", len(df_val))\n",
    "print(\"Testing Set Size: \", len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原本version用processed_dataset裡面的資料，y = 0 and 1的筆數過於懸殊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_labeled_train = load_pickle('newsapi-dataset/processed_dataset/IF_NewsDataSet_Train.pkl')\n",
    "# df_labeled_val = load_pickle('newsapi-dataset/processed_dataset/IF_NewsDataSet_Val.pkl')\n",
    "# df_labeled_test = load_pickle('newsapi-dataset/processed_dataset/IF_NewsDataSet_Test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_labeled_train.dropna(subset=['content'], inplace = True)#['content'] == None\n",
    "# df_labeled_val.dropna(subset=['content'], inplace = True)\n",
    "# df_labeled_test.dropna(subset=['content'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    實作一個可以用來讀取訓練 / 測試集的 Dataset，此 Dataset 每次將 tsv 裡的一筆成對句子\n",
    "    轉換成 BERT 相容的格式，並回傳 3 個 tensors：\n",
    "    - tokens_tensor：兩個句子合併後的索引序列，包含 [CLS] 與 [SEP]\n",
    "    - segments_tensor：可以用來識別兩個句子界限的 binary tensor\n",
    "    - label_tensor：將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "   \n",
    "class NewsDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    # mode: in [\"train\", \"test\", \"val\"]\n",
    "    # path: if given, then read data from the path(ex training set)\n",
    "    def __init__(self, df, mode, tokenizer, perc = 70, path = None):\n",
    "        assert mode in [\"train\", \"val\", \"test\"]  # 一般訓練你會需要 dev set\n",
    "#         MAX_LENGTH = 3000\n",
    "#         df = df[~(df['content'].apply(lambda x : len(x)) > MAX_LENGTH)]\n",
    "        print(len(df))\n",
    "        self.df = df.reset_index()\n",
    "        self.mode = mode\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer \n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    #@pysnooper.snoop()  # 加入以了解所有轉換過程\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text = self.df.loc[idx, 'content']\n",
    "            label_tensor = None\n",
    "        elif self.mode == \"val\":\n",
    "            label, text = self.df.loc[idx, ['scores', 'content']].values\n",
    "            label_tensor = torch.tensor(label)\n",
    "        else:\n",
    "            label, text = self.df.loc[idx, ['scores', 'content']].values\n",
    "            # 將label文字也轉換成索引方便轉換成 tensor\n",
    "            label_tensor = torch.tensor(label)\n",
    "            \n",
    "        \"\"\"\n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(text_a)\n",
    "        word_pieces += tokens_a + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        # 第二個句子的 BERT tokens\n",
    "        tokens_b = self.tokenizer.tokenize(text_b)\n",
    "        word_pieces += tokens_b + [\"[SEP]\"]\n",
    "        len_b = len(word_pieces) - len_a\n",
    "        \"\"\"\n",
    "        # 建立句子的 BERT tokens \n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        word_pieces += tokens[:62] + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        segments_tensor = torch.tensor([1] * len_a, dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "384\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "# 初始化一個專門讀取樣本的 Dataset，使用 'bert-base-uncased' 斷詞\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "trainset = NewsDataset(df_train, \"train\", tokenizer=tokenizer)\n",
    "valset = NewsDataset(df_val, \"val\", tokenizer=tokenizer)\n",
    "testset = NewsDataset(df_test, \"test\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作可以一次回傳一個 mini-batch 的 DataLoader\n",
    "這個 DataLoader 吃我們上面定義的 `OnlineQueryDataset`，\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 這個函式的輸入 `samples` 是一個 list，裡頭的每個 element 都是\n",
    "# 剛剛定義的 `FakeNewsDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "# 它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 訓練集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一個每次回傳 64 個訓練樣本的 DataLoader\n",
    "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
    "BATCH_SIZE = 64\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True,  \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE,  \n",
    "                         collate_fn=create_mini_batch, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([64, 64]) \n",
      "tensor([[  101, 15476,  2756,  ...,  2779,  3123,   102],\n",
      "        [  101,  1006, 22950,  ..., 26698,  2006,   102],\n",
      "        [  101,  2899,  1006,  ...,  1997,  3688,   102],\n",
      "        ...,\n",
      "        [  101,  5522,  1010,  ...,  1037, 26665,   102],\n",
      "        [  101,  4199,  1006,  ...,  1996,  2223,   102],\n",
      "        [  101,  4638,  2041,  ...,  2048,  3316,   102]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([64, 64])\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([64, 64])\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([64])\n",
      "tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, \\\n",
    "    masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRETRAINED_MODEL_NAME = \"model_berttokenizer\"\n",
    "PRETRAINED_MODEL_NAME = \"bert_model/newsbert_owntokenizer\"\n",
    "NUM_LABELS = 2 # 0 or 1\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo rm -rf ~/.nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "classification acc: 0.5\n"
     ]
    }
   ],
   "source": [
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "pred, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train 下游任務 （對trader有沒有用的classifier）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "loss: 2.569, acc: 0.767\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.16\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "loss: 1.874, acc: 0.511\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "loss: 1.708, acc: 0.622\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.05\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "loss: 1.630, acc: 0.900\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.81\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "loss: 1.402, acc: 0.839\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "# 使用 Adam Optim 更新整個分類模型的參數\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, EPOCHS))\n",
    "    print('Training...')\n",
    "\n",
    "    # 訓練模式\n",
    "    model.train()\n",
    "\n",
    "    for data in trainloader: # trainloader is an iterator over each batch\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    \n",
    "    # 計算分類準確率\n",
    "    logit, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "    print('loss: %.3f, acc: %.3f' %\n",
    "          (running_loss, acc))    \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for data in valloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "            \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "\n",
    "    _, acc = get_predictions(model, valloader, compute_acc=True)\n",
    "        \n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 比較用的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"clf_model/pre_bert/first64tokenep25newsplit\", num_labels=2)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to clf_model/news_bert_own_tokenizer/first64tokenep25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('clf_model/news_bert_own_tokenizer/first64tokenep25/vocab.txt',\n",
       " 'clf_model/news_bert_own_tokenizer/first64tokenep25/special_tokens_map.json',\n",
       " 'clf_model/news_bert_own_tokenizer/first64tokenep25/added_tokens.json')"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "output_dir = os.path.join(\"clf_model\", \"news_bert_own_tokenizer/first64tokenep25\")\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立測試集。這邊我們可以用跟訓練時不同的 batch_size，看你 GPU 多大\n",
    "testloader = DataLoader(testset, batch_size=32, collate_fn=create_mini_batch)\n",
    "\n",
    "# 用分類模型預測測試集\n",
    "predictions = get_predictions(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_predictions(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(label, pred):\n",
    "    return (label == pred).sum().item()/len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = torch.cuda.FloatTensor(testset.df['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset accuracy: 0.794271\n"
     ]
    }
   ],
   "source": [
    "print(\"Testset accuracy: %f\" % accuracy(test_label, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[297,  76],\n",
       "       [  3,   8]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_label.cpu(), predictions.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我們的News Bert(with pretrained bert base uncased tokenizer)\n",
    "| Model | acc(%)  | recall(%) | precision(%) | CM |\n",
    "|-------|:-----|:-------|:----------|:---|\n",
    "|64token_oldsplit|98|18|40|[370,3][9,2]|\n",
    "|32token_newsplit|80|64|9|[299,74][4,7]|\n",
    "|   ||   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用Pretrained Bert(bert base uncased)\n",
    "| Model | acc(%)  | recall(%) | precision(%) | CM |\n",
    "|-------|:-----|:-------|:----------|:---|\n",
    "|32newsplit|81|100|13|[299,74][0,11]|\n",
    "|64newsplit|85|72|13|[317,56][3,8]|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用我們的News Bert(加上自己train的tokenizer)\n",
    "| Model | acc(%)  | recall(%) | precision(%) | CM |\n",
    "|-------|:-----|:-------|:----------|:---|\n",
    "|64newsplit|79|72|10|[297,76][3,8]|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09523809523809523"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8/84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Maduro’s',\n",
       " 'government',\n",
       " 'denies',\n",
       " 'there',\n",
       " 'is',\n",
       " 'an',\n",
       " 'economic',\n",
       " 'crisis',\n",
       " 'and',\n",
       " 'has',\n",
       " 'said',\n",
       " 'soldiers',\n",
       " 'would',\n",
       " 'remain',\n",
       " 'stationed',\n",
       " 'along',\n",
       " 'the',\n",
       " 'country’s',\n",
       " 'borders',\n",
       " 'to',\n",
       " 'prevent',\n",
       " 'potential',\n",
       " 'incursions.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.796875"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(299 + 7)/(299 + 7 + 74 + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def preprocess(doc):\n",
    "    # stop_words\n",
    "    #stop_words = stopwords.words('english')\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
    "                  'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", \n",
    "                  'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "                  \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "                  'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', \n",
    "                  'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', \n",
    "                  'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', \n",
    "                  'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', \n",
    "                  'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", \n",
    "                  'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", \n",
    "                  'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', \n",
    "                  'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", \n",
    "                  'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    # stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_doc = []\n",
    "    if doc:   \n",
    "        doc = doc.lower()\n",
    "        doc = word_tokenize(doc)\n",
    "        doc = [w for w in doc if not w in stop_words]\n",
    "        doc = [w for w in doc if w.isalpha()]\n",
    "\n",
    "        for w in doc:\n",
    "            #pro_w = lemmatize_stemming(w)\n",
    "            pro_w = stemmer.stem(WordNetLemmatizer().lemmatize(w, pos='v'))\n",
    "            processed_doc.append(pro_w)\n",
    "    else:\n",
    "        processed_doc.append('')      \n",
    "    return \" \".join(processed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r model_berttokenizer gs://luckykcw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
